# ***我的机器学习支线***

[toc]



## **机器学习方法定义**

**根据机器学习时的信号、目标和思想可将机器学习分为监督学习、无监督学习和强化学习**



### **监督学习**

**学习时，给机器提供输入数据所需的输出数据的示例即标签，信号是输出数据的反馈，目标是学习将输入映射到输出的一般规则，思想是归纳**

- **监督学习的类型有分类与回归**

  > *当输出被限制为一组有限的值时，使用分类算法，当输出可能具有某个范围内的任何数值时，使用回归算法* 



#### **分类**

**量化观察结果为解释变量或特征的属性，从而分离观察结果**



##### **多类分类**

- **实例分类为两类之一称为二元分类**

- **多类分类或称多项式分类将实例分类为三个或更多类之一，每个样本有且仅有一个标签，实例只可分配到单个类**

  > *例如，一堆水果图片分类，一个水果可以是橘子或苹果或梨，但是同时不可能是三者* 



##### **多标签分类**

- **多标签分类是多类分类的泛化，每个样本可以有多个标签，实例可以分配到多个类**

  > *例如，一些文本话题分类，它们可能被同时认为是宗教、政治、金融和教育相关的* 



##### **多输出-多类分类**

- **多输出-多类分类是多标签分类的泛化，每个样本包含多个标签，每个标签有多种取值，实例可以分配到多个类，每个类又有多种输出，通常表示为 $2d$  阵列**

  > *例如，去除 $8bit$ 图片中的噪点，输入一张包含噪点的图片，然后输出一张干净的图片，样本是像素点强度矩阵，每个像素点就是一个类，且每个类可以有中输出即像素点的像素强度从 $0～255$，实例可分配到多个类的多种输出* 



##### **多任务分类**

- **多任务分类与多输出-多类分类相似，重点在于认为不同类是不同任务**

- **不等价于通过多任务学习 (*Multi-task learning*) 实现分类，广义上使用多种损失函数就可以认为是多任务学习，因为多种损失函数具有多任务的先验信息**

  > *例如，垃圾邮件分类，说英语的人可能会认为所有法语的电子邮件是垃圾邮件，说法语的人则反之，同时，所有邮件中都可能认为与汇款相关的邮件不是垃圾邮件，通过这两个不同的任务来学习实现垃圾邮件分类就是多任务学习的方式实现二元分类* 



### **无监督学习**

**学习时，不给机器提供输入数据所需的标签，信号是数据中的共性，目标时让机器自动找到输入数据中的结构，思想是归纳**



- **无监督学习的主要类型是聚类**

  > *传统的方法有层次聚类、K-means 聚类、基于分布的聚类、基于密度的聚类和基于网格的聚类等* 

  > *基于神经网络的方法有玻尔兹曼机、受限玻尔兹曼机、亥姆霍兹、自编码器、变分自编码器和生成对抗网络等*

- **无监督学习的一个核心应用是统计学中的密度估计**

  > *寻找概率密度函数*



### **强化学习**

**学习时，机器与必须执行特定目标例如驾驶车的动态马尔可夫决策过程，信号是不同决策的奖励，目标是找到一个好的策略即最大化奖励，思想是演化**

- **马尔可夫决策过程是一种离散时间随机控制过程**

  > *马尔可夫决策过程是马尔可夫链的扩展，不同之处在于每个状态允许选择动作并且具有奖励。反过来说，如果每个状态只存在一个动作例如等待并且所有奖励都相同例如零，则马尔可夫决策过程简化为马尔可夫链* 

- **马尔可夫决策过程表示为 ${\displaystyle (S,A,P_{a},R_{a})}$**

  > ${\displaystyle S}$ 是一组状态的空间
  >
  > ${\displaystyle A}$ 是一组动作的空间（${\displaystyle A_{s}}$ 是状态 ${\displaystyle s}$ 的一组动作）
  >
  > ${\displaystyle P_{a}(s,s')=\Pr(s_{t+1}=s'\mid s_{t}=s,a_{t}=a)}$ 是 ${\displaystyle t}$ 时刻的状态为 ${\displaystyle s}$ 动作为 ${\displaystyle a}$ 时导致 ${\displaystyle t+1}$ 时状态为 ${\displaystyle s'}$ 的概率
  >
  > ${\displaystyle R_{a}(s,s')}$ 是从 ${\displaystyle s}$ 由于动作 ${\displaystyle a}$ 转换为 ${\displaystyle s'}$ 状态后收到奖励
  >
  > 策略是最大化 ${\displaystyle E\left[\sum _{t=0}^{\infty }{\gamma ^{t}R_{a_{t}}(s_{t},s_{t+1})}\right ]},\;0\leq\gamma\leq1$ 

- **优化过程通过价值 $V(s)$ 更新与策略 $\pi(s)$ 迭代更新实现**

  > $$
  > \begin{aligned}
  > V(s) :=& \sum_{s'} P_{\pi(s)} (s,s') \left( R_{\pi(s)} (s,s') + \gamma V(s')\right)\\
  > \pi (s):=&\;\operatorname {argmax} a\left\{\sum _{s'}P_{a}(s,s')\left(R_{a}(s, s')+\gamma V(s')\right)\right\}
  > \end{aligned}
  > $$



## **损失函数**

**损失函数或代价函数是将随机事件或其有关随机变量的取值映射为非负实数以表示该随机事件的损失或风险的函数。损失函数通常作为学习准则与优化问题相联系，即通过最小化损失函数求解和评估模型。在机器学习中被用于模型的参数估计**



### **语义分割**

**语义分割结合了图像分类、目标检测和图像分割，通过一定的方法将图像分割成具有一定语义含义的区域块，并识别出每个区域块的语义类别，实现从底层到高层的语义推理过程，最终得到一幅具有逐像素语义标注的分割图像。设计损失函数想要达到的目标是损失与梯度同步变化，求导自变量定义为神经网络的最后一层带权重层的输出。当学习率恒定时，希望当预测结果远离真实值时，损失大，梯度大；当预测结果靠近真实值时，损失小，梯度小**



#### **交叉熵损失**

**最常用损失函数是像素级别的交叉熵损失 *(cross entropy loss，ce)*，逐个检查每个像素，将对每个像素类别的预测结果（概率分布向量）与热编码标签向量进行比较**

**假设我们需要对每个像素的预测类别有 $5$ 个，则预测的概率分布向量长度也为 $5$ 维**

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/640-20220524102921096.jpeg" alt="图片" style="zoom:50%;" />

**对应的每个像素损失函数**
$$
\pmb{loss_{pixel}}=-\sum_{class}y_{true}^{class}log(y_{pred}^{class})
$$
**令 $y_{pred}=softmax(x)$ 那么回传的梯度为 $\frac{d(loss_{ce})}{dx}=\sum_{class}y_{true}^{class}(y_{pred}^{class}-1)$ 正比于每个类别误差求和的均值，因此优化过程中损失小时梯度小**

**整个图像的损失就是全部像素损失的平均值**
$$
\pmb{loss_{ce}}=\frac{1}{n}\sum_{pixel=1}^{n}loss_{pixel}
$$
**特别注意 *(binary entropy loss，bce)* 是针对只有两个类别的情况的损失函数**
$$
\pmb{loss_{bce}}=-y_{true}log(y_{pred})-(1-y_{true})log(1-y_{pred})
$$
**如果 $y_{pred}=sigmoid(x)$ 那么回传的梯度为 $\frac{d(loss_{bce})}{dx}=y_{pred}-y_{true}$ 正比于误差，因此优化过程中损失小时梯度小**



#### **Weighted Loss**

**交叉熵损失会分别评估每个像素的类别预测，然后对所有像素的损失进行平均，因此实质上是在对图像中的每个像素进行平等地学习。如果多个类在图像中的分布不均衡，那么这可能导致训练过程由像素数量多的类所主导，即模型会主要学习数量多的类别样本的特征，并且学习出来的模型会更偏向将像素预测为该类别**

**全卷积神经网络 *FCN* 与 *U* 型神经网路 *U-Net* 论文中对输出概率分布向量中的每个值进行加权，使得模型更加关注数量较少的样本，以缓解图像中存在的类别不均衡问题**

**例如，二分类中正负样本比例为 $1:99$，此时模型将所有样本都预测为负样本，那么准确率仍有 $99\%$，然而实际上没有意义**

**为了平衡这个差距，就对正样本和负样本的损失赋予不同的权重，带权重的二分类损失函数 *weighted loss***
$$
\pmb{loss_{wieghted}}=-pos_{wieghted}\times y_{true}log(y_{pred})-(1-y_{true})log(1-y_{pred})\\
\pmb{pos_{wieghted}=\frac{neg_{num}}{pos_{num}}}
$$
**令 $y_{pred}=sigmoid(x)$ 那么回传的梯度为 $\frac{d(loss_{wieghted})}{dx}=(1-y_{true})y_{pred}-pos_{wieghted}\times y_{true}(1-y_{pred})$ 正比于误差，且正样本则为 $pos_{wieghted}(y_{pred}-1)$ 被抑制，负样本则为 $-y_{pred}$ 相对增强，因此优化过程中损失小时梯度小，且放大了负样本的优化效果**



#### **Focal Loss**

**有时不仅需要针对不同类别的像素数量的不均衡改进，还需要将像素分为难学习和容易学习这两种样本，对于容易学习的样本模型可以很轻松地预测正确，而模型只要将大量容易学习的样本预测正确，*loss* 就会减小很多，从而导致模型无法顾及难学习的样本，所以要让模型更加关注难学习的样本**

**对于难易程度不同的学习样本可赋予不同的权重调整**
$$
-(1-y_{pred})^{\gamma}\times y_{true}log(y_{pred})-y_{pred}^{\gamma}(1-y_{true})\times log(1-y_{pred})\\
default\;\gamma=2
$$
**例如，预测一个正样本，预测结果为 $0.95$ 是一个容易学习的样本，有 $(1-0.95)^2=0.0025$ 损失直接减少为原来的 $1\over400$，预测结果为 $0.5$ 是一个难学习的样本，有 $(1-0.5)^2=0.25$，损失减小为原来的 $1\over4$，相对减小的程度小很多，总体上更多的考虑到了难学习样本，因此模型更加专注学习难学习的样本**

**可得考虑正负样本不均衡与难易程度的 *focal loss***
$$
\pmb{loss_{focal}}=-\alpha(1-y_{pred})^{\gamma}\times y_{true}log(y_{pred})-(1-\alpha)y_{pred}^{\gamma}(1-y_{true})\times log(1-y_{pred})\\
\pmb{default\;\gamma=2}
$$
**梯度性质于 *Weighted Loss* 类似**



#### **Soft Dice Loss**

**常用的损失函数还有基于 $Dice$ 系数的损失函数 *(soft dice loss，sd)* 其系数实质是两个样本之间重叠的度量，范围为 $0～1$，其中 $1$ 表示完全重叠**
$$
Dice=\frac{2|A\cap B|}{|A|+|B|}=\frac{2TP}{2TP+FP+FN}
$$
**$|A\cap B|$ 代表集合 $A$ 和 $B$ 之间的公共元素，并且 $|A|$ 与 $|B|$ 分别代表集合 $A$ 和 $B$ 的元素数量，分子乘 $2$ 保证取值范围在 $[0,1]$，$|A\cap B|$ 为预测掩码和标签掩码之间的逐元素乘法，然后对结果矩阵求和**

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/640.jpeg" alt="图片" style="zoom:50%;" />

**$Dice$ 系数中 $TP$ 为真阳性样本 $FP$ 为假阳性样本 $FN$ 为假阴性样本，而 $precision=\frac{TP}{TP+FP}$，$recall=\frac{TP}{TP+FN}$，可知 $Dice$ 包涵了两部分的意义**

**对于神经网络的输出，分子与我们的预测和标签之间的共同激活有关，而分母分别与每个掩码中的激活数量有关，具有根据标签掩码的尺寸对损失进行归一化的效果**

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/640-20220524102921043.jpeg" alt="图片" style="zoom: 50%;" />

**对每个类别都计算 $1-Dice$ 后求和取平均得到最后的 *soft dice loss***
$$
\pmb{loss_{sd}}=\frac{1}{n}\sum_{class=1}^{n}\left\{1-\frac{2\sum_{piexl}(y_{true}y_{pred})}{\sum_{piexl}(y_{true}+y_{pred})}\right\}
$$
**如果是二分类则令 $y_{pred}=sigmoid(x)$ 那么回传的梯度为**
$$
\frac{d(loss_{sd}^{pixel})}{dy^{pixel}}=\frac{1}{2}\sum_{class=1}^{2}\left\{\frac{2[y_{true}^{pixel}(y_{true}^{pixel}+y_{pred}^{pixel})-y_{true}^{pixel}y_{pred}^{pixel}]}{(y_{true}^{pixel}+y_{pred}^{pixel})^2}\right\}=\frac{1}{2}\sum_{class=1}^{2}
\begin{cases}
0&,y_{true}^{pixel}=0\\
\frac{-2}{(1+y_{pred}^{pixel})^2}&,y_{true}^{pixel}=1
\end{cases}
$$

$$
\frac{d(loss_{sd}^{pixel})}{dx^{pixel}}=\frac{d(loss_{sd}^{pixel})}{dy^{pixel}}\times\frac{e^{-x^{pixel}}}{(e^{-x^{pixel}}+1)^2}
$$

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/output-3396439.svg" alt="output" style="zoom: 67%;" />

**随着 $x^{pixel}$ 增大，损失（蓝色）趋向零梯度（红色）趋向零，随着 $x^{pixel}$ 减小，损失趋于一梯度趋向零（类似均方误差 *(mse)* 不论预测接近真实值或是接近错误值，梯度都很小）**



#### **Soft IoU Loss**

**计算 $Dice$ 系数的公式也可以表示为**
$$
Dice=\frac{2TP}{2TP+FP+FN}
$$


**$IoU$ 的计算公式和这个很像，区别仅只计算一次 $TP$** 
$$
IoU=\frac{TP}{TP+FP+FN}=\frac{|A\cap B|}{|A|+|B|-|A\cap B|}
$$
**对于每个类别的 *mask* 都计算 $1-IoU$ 最后求和取平均得到基于 $IoU$ 系数的损失函数 *(soft iou loss，si)* 为**
$$
\pmb{loss_{si}}=\frac{1}{n}\sum_{class=1}^{n}\left\{1-\frac{\sum_{piexl}(y_{true}y_{pred})}{\sum_{piexl}(y_{true}+y_{pred}-y_{true}y_{pred})}\right\}
$$
**梯度性质于 *soft dice loss* 类似**



#### **小结**

**交叉熵损失把每个像素都当作一个独立样本进行预测，而 *soft dice loss* 与 *soft iou loss* 则以更整体的方式来看待最终的预测输出，两类损失是针对不同情况，各有优点和缺点，在实际应用中，可以同时使用这两类损失来进行互补**

**参考**

- [语义分割中的 loss function 最全面汇总](https://mp.weixin.qq.com/s/GkiV_KMWLcxiFnqFavtuwA)
- [An overview of semantic image segmentation](https://www.jeremyjordan.me/semantic-segmentation)
- [Loss Functions for Medical Image Segmentation](https://medium.com/@junma11/loss-functions-for-medical-image-segmentation-a-taxonomy-cefa5292eec0)
- [Losses for Image Segmentation](https://lars76.github.io/neural-networks/object-detection/losses-for-segmentation)



## **模型复杂度**

**模型复杂度通常是指前向过程的计算量（反映模型所需要的计算时间）和参数个数（反映模型所需要的计算机内存空间）**



### **时间复杂度**

**用于评价模型运行效率高低，通常意味着模型运行速度**

- 计算复杂度使用浮点运算数 *FLOPs*  

- 另外并行性也会影响模型运行速度，可使用最大顺序操作数 *Minimum number of sequential operations* 和吞吐量 *Throughput (image/s)* 以及推理时间 *Inference time (bacth/ms)* 衡量

  > 其中吞吐量与推理时间不仅仅与模型有关，还与硬件性能有关



#### ***FLOPs***

##### ***1.  Convolution***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/%E5%8D%B7%E7%A7%AF.jpeg" alt="卷积" width="600" />
$$
FLOPs=(2\times C_{input}\cdot S_{filter_h}\cdot S_{filter_w}-1)^*\cdot C_{output}\cdot S_{input_h}\cdot S_{input_w}\\
\begin{aligned}\\
e.g.\quad 
&C_{input}=3\quad C_{output}=4\quad S_{filter_h}=S_{filter_w}=3\quad S_{input_h}=S_{input_w}=6\\
&FLOPs=(2\times3\times3^2-1)\times4\times6^2=7632
\end{aligned}
$$
**\* 卷积有偏置则不需要 *-1*** 



##### ***2.  Attention***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/%E6%B3%A8%E6%84%8F%E5%8A%9B.png" alt="image-20220122113102360" width="600" />
$$
FLOPs=\begin{cases}
2D_kND_x\;+\;2D_kN^2\;+\;1\\

3D^2N\;+\;2DN^2\;+\;1\quad if\quad D_x=D_k=D_v=Our\,D_{model}=D
\end{cases}
$$

##### ***3.  Fully connected***

**假设全连接包括输入层隐含层输出层三层，输入层包含 *N* 批次 *D* 个神经元，隐含层包含 *N* 批次 *4D* 个神经元，输出层包含 *N* 批次 *D* 个神经元**
$$
\begin{align}\\
FLOPs\;&=\;D\cdot 4D\cdot N+4D\cdot D\cdot N\\
&=\;8D^2N

\end{align}
$$

### **空间复杂度**

**用于评价模型占用空间大小，通常意味着模型能否运行** 

- 参数量 *Parameters* 
- 数据位数 *Data bits* 

#### ***Parameters***

$$
Parameters=Volume(Tensor_{Wight})
$$

#### ***Data bits***

$$
Float32\quad or\quad Float64
$$

### **深度学习模型调研**

#### ***0.  Attention Is All You Need***

**Per-layer complexity, minimum number of sequential operations for different layer types and maximum path length** 

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/image-20220121125955712.png" alt="image-20220121125955712" width="1000" />

**$n$ 是 sequence length、$d$ 是 representation dimension、$k$ 是卷积核尺寸和 $r$ 受限自注意力机制的领域尺寸**

**首次提出完全基于注意力和全联接的 *Transformer* 架构的自然语言处理神经网络，*maximum path length* $O(x)$ 其 $x$ 越大代表在长距离依赖的结点传递信息时，信息交互越难，信息丢失越严重** 



#### ***1.  Densely Connected Convolutional Networks***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/image-20220120211612502.png" alt="image-20220120211612502" width="1000" />

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/image-20220120204504919.png" alt="image-20220120204504919" width="1000" />

**具有 *BottleNeck* 结构的 *DenseNet- L*$(k=n)$ ，** ***L* 代表模型深度，即可学习的层数（卷积层与全连接层）*$k$* 为输入的 *feature* 经过一个 *Dense Block* 中的一个 *Dense Layer* 后增加的特征通道数，经过一个 *Dense Block* 后，紧接着的 *Transition Layer* 后会将当前 *feature* 的特征通道数压缩一半** 

**“If a dense block contains m feature-maps, we let the following transition layer generate $⌊ θm ⌋$ output featuremaps, where $0 <θ ≤ 1$ is referred to as the compression factor.”**

**“We refer the DenseNet with $θ<1$ as DenseNet-C, and we set $θ = 0.5$ in our experiment. When both the bottleneck and transition layers with $θ < 1$ are used, we refer to our model as DenseNet-BC.”**



#### ***2.  Deep Residual Learning for Image Recognition***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/image-20220121203650386.png" alt="image-20220121203650386" width="1000" />

**其中 *FLOPs* 被误为 *MACs*，实际 *FLOPs* 应该是上述的两倍大小，*L-layer* 中 *L* 代表可学习的层数** 

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/image-20220120205312977.png" alt="image-20220120205312977" width="600" />

**加入 *bottleneck* 结构后网络参数量明显下降，实现了超过 *1000* 层的网络** 



#### ***3.  https://github.com/sovrasov/flops-counter.pytorch***

**通过调用外部库 *flops-counter* 计算的主流卷积模型的参数量和乘加操作数，并相应给出了 *Top1* 和 *Top5* 精度** 

|       Model        | Input Resolution | Params(M) | MACs(G) | Acc@1  | Acc@5  |
| :----------------: | :--------------: | :-------: | :-----: | :----: | :----: |
|   ***alexnet***    |     224x224      |   61.1    |  0.72   | 56.432 | 79.194 |
| ***densenet121***  |     224x224      |   7.98    |  2.88   | 74.646 | 92.136 |
| ***densenet161***  |     224x224      |   28.68   |  7.82   | 77.56  | 93.798 |
| ***densenet169***  |     224x224      |   14.15   |  3.42   | 76.026 | 92.992 |
| ***densenet201***  |     224x224      |   20.01   |  4.37   | 77.152 | 93.548 |
|    ***dpn107***    |     224x224      |   86.92   |  18.42  | 79.746 | 94.684 |
|    ***dpn131***    |     224x224      |   79.25   |  16.13  | 79.432 | 94.574 |
|    ***dpn68***     |     224x224      |   12.61   |  2.36   | 75.868 | 92.774 |
|    ***dpn68b***    |     224x224      |   12.61   |  2.36   | 77.034 | 93.59  |
|    ***dpn92***     |     224x224      |   37.67   |  6.56   |  79.4  | 94.62  |
|    ***dpn98***     |     224x224      |   61.57   |  11.76  | 79.224 | 94.488 |
| ***inceptionv3***  |     299x299      |   27.16   |  5.73   | 77.294 | 93.454 |
| ***inceptionv4***  |     299x299      |   42.68   |  12.31  | 80.062 | 94.926 |
|  ***resnet101***   |     224x224      |   44.55   |  7.85   | 77.438 | 93.672 |
|  ***resnet152***   |     224x224      |   60.19   |  11.58  | 78.428 | 94.11  |
|   ***resnet18***   |     224x224      |   11.69   |  1.82   | 70.142 | 89.274 |
|   ***resnet34***   |     224x224      |   21.8    |  3.68   | 73.554 | 91.456 |
|   ***resnet50***   |     224x224      |   25.56   |  4.12   | 76.002 | 92.98  |
| ***se_resnet101*** |     224x224      |   49.33   |  7.63   | 78.396 | 94.258 |
| ***se_resnet152*** |     224x224      |   66.82   |  11.37  | 78.658 | 94.374 |
| ***se_resnet50***  |     224x224      |   28.09   |   3.9   | 77.636 | 93.752 |
|    ***vgg11***     |     224x224      |  132.86   |  7.63   | 68.97  | 88.746 |
|    ***vgg13***     |     224x224      |  133.05   |  11.34  | 69.662 | 89.264 |
|    ***vgg16***     |     224x224      |  138.36   |  15.5   | 71.636 | 90.354 |
|    ***vgg19***     |     224x224      |  143.67   |  19.67  | 72.08  | 90.822 |



#### ***4.  AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/image-20220122152835106.png" alt="image-20220122152835106" width="600" />

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/image-20220122154046936.png" alt="image-20220122154046936" width="600" />

***VIT* 完全基于注意力机制和全连接的视觉神经网络**



#### ***5.  Swin Transformer: Hierarchical Vision Transformer using Shifted Windows***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/image-20220122152002688.png" alt="image-20220122152002688" width="1000" />

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/image-20220122153651890.png" alt="image-20220122153651890" width="600" />

***Swin* 完全基于具有滑动窗口的注意力机制和全连接的视觉神经网络**



### **散射成像领域的模型对比**

***以下的计算 Batch 统一设置为 2***

#### ***1.  Deep speckle correlation: a deep learning approachtoward scalable imaging through scattering media***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/image-20220122144343659.png" alt="image-20220122144343659" width="600" />

- ***Input Resolution* : $256\times 256$**

- ***Parameters* : $21.8505\times 10^6$** 
- ***FLOPs* : $0.0577\times 10^9$ ** 
- ***Throughput* :** $8.9\,image/s$
- ***Inference time* :** $223.2022\,batch/ms$

#### ***2.  High-generalization deep sparse pattern reconstruction: feature extraction of speckles using self-attention armed convolutional neural networks***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/image-20220122144525611.png" alt="image-20220122144525611" width="600" />

##### ***SA-CNN***

- ***Input Resolution* : $256\times 256$**
- ***Parameters* : $13.9231\times 10^6$** 
- ***FLOPs* : $17.4204\times 10^9$ ** 
- ***Throughput* :** $40.8\,image/s$
- ***Inference time* :** $49.0446\,batch/ms$

##### ***SA-CNN-Single\****

- ***Input Resolution* : $256\times 256$**
- ***Parameters* : $13.5972\times 10^6$** 
- ***FLOPs* : $8.9002\times 10^9$ ** 
- ***Throughput* :** $44.4\,image/s$
- ***Inference time* :** $45.0413\,batch/ms$

**\* 仅有中间一层注意力** 



#### ***3.  Our SpT UNet***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%88%91%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%94%AF%E7%BA%BF/SpT%20UNet.png" alt="Xnip2022-01-22_15-06-45" width="600" />

##### ***SpT UNet***

- ***Input Resolution* : $200\times 200\quad 224\times 224\quad 256\times 256$**
- ***Parameters* : $6.6184\times 10^6$** 
- ***FLOPs* : $19.3602\times 10^9\quad 24.2856\times 10^9\quad 31.7197\times 10^9$ **
- ***Throughput* :** $86.9\,image/s\quad 83.3\,image/s\quad 62.5\,image/s\quad$
- ***Inference time* :** $23.0214\,batch/ms\quad 24.0215\,batch/ms\quad 31.3427\,batch/ms$

##### ***SpT UNet-B\****

- ***Input Resolution* : $200\times 200\quad 224\times 224\quad 256\times 256$**
- ***Parameters* : $2.4179\times 10^6$** 
- ***FLOPs* : $8.2659\times 10^9\quad 16.2256\times 10^9\quad 21.2318\times 10^9$ **
- ***Throughput* :** $105.2\,image/s\quad 95.2\,image/s\quad 72.9\,image/s\quad$
- ***Inference time* :** $19.0217\,batch/ms\quad 21.0189\,batch/ms\quad 27.4584\,batch/ms$

**\* *puffed* 下采样和 *leaky* 上采样采用 *Bottleneck* 结构** 

