# ***我的机器学习支线***

[toc]



## **机器学习方法定义**

**根据机器学习时的信号、目标和思想可将机器学习分为监督学习、无监督学习和强化学习**



### **监督学习**

**学习时，给机器提供输入数据所需的输出数据的示例即标签，信号是输出数据的反馈，目标是学习将输入映射到输出的一般规则，思想是归纳**

- **监督学习的类型有分类与回归**

  > *当输出被限制为一组有限的值时，使用分类算法，当输出可能具有某个范围内的任何数值时，使用回归算法* 



#### **分类**

**量化观察结果为解释变量或特征的属性，从而分离观察结果**



##### **多类分类**

- **实例分类为两类之一称为二元分类**

- **多类分类或称多项式分类将实例分类为三个或更多类之一，每个样本有且仅有一个标签，实例只可分配到单个类**

  > *例如，一堆水果图片分类，一个水果可以是橘子或苹果或梨，但是同时不可能是三者* 



##### **多标签分类**

- **多标签分类是多类分类的泛化，每个样本可以有多个标签，实例可以分配到多个类**

  > *例如，一些文本话题分类，它们可能被同时认为是宗教、政治、金融和教育相关的* 



##### **多输出-多类分类**

- **多输出-多类分类是多标签分类的泛化，每个样本包含多个标签，每个标签有多种取值，实例可以分配到多个类，每个类又有多种输出，通常表示为 $2d$  阵列**

  > *例如，去除 $8bit$ 图片中的噪点，输入一张包含噪点的图片，然后输出一张干净的图片，样本是像素点强度矩阵，每个像素点就是一个类，且每个类可以有中输出即像素点的像素强度从 $0～255$，实例可分配到多个类的多种输出* 



##### **多任务分类**

- **多任务分类与多输出-多类分类相似，重点在于认为不同类是不同任务**

- **不等价于通过多任务学习 (*Multi-task learning*) 实现分类，广义上使用多种损失函数就可以认为是多任务学习，因为多种损失函数具有多任务的先验信息**

  > *例如，垃圾邮件分类，说英语的人可能会认为所有法语的电子邮件是垃圾邮件，说法语的人则反之，同时，所有邮件中都可能认为与汇款相关的邮件不是垃圾邮件，通过这两个不同的任务来学习实现垃圾邮件分类就是多任务学习的方式实现二元分类* 



### **无监督学习**

**学习时，不给机器提供输入数据所需的标签，信号是数据中的共性，目标时让机器自动找到输入数据中的结构，思想是归纳**



- **无监督学习的主要类型是聚类**

  > *传统的方法有层次聚类、K-means 聚类、基于分布的聚类、基于密度的聚类和基于网格的聚类等* 

  > *基于神经网络的方法有玻尔兹曼机、受限玻尔兹曼机、亥姆霍兹、自编码器、变分自编码器和生成对抗网络等*

- **无监督学习的一个核心应用是统计学中的密度估计**

  > *寻找概率密度函数*



### **强化学习**

**学习时，机器与必须执行特定目标例如驾驶车的动态马尔可夫决策过程，信号是不同决策的奖励，目标是找到一个好的策略即最大化奖励，思想是演化**

- **马尔可夫决策过程是一种离散时间随机控制过程**

  > *马尔可夫决策过程是马尔可夫链的扩展，不同之处在于每个状态允许选择动作并且具有奖励。反过来说，如果每个状态只存在一个动作例如等待并且所有奖励都相同例如零，则马尔可夫决策过程简化为马尔可夫链* 

- **马尔可夫决策过程表示为 ${\displaystyle (S,A,P_{a},R_{a})}$**

  > ${\displaystyle S}$ 是一组状态的空间
  >
  > ${\displaystyle A}$ 是一组动作的空间（${\displaystyle A_{s}}$ 是状态 ${\displaystyle s}$ 的一组动作）
  >
  > ${\displaystyle P_{a}(s,s')=\Pr(s_{t+1}=s'\mid s_{t}=s,a_{t}=a)}$ 是 ${\displaystyle t}$ 时刻的状态为 ${\displaystyle s}$ 动作为 ${\displaystyle a}$ 时导致 ${\displaystyle t+1}$ 时状态为 ${\displaystyle s'}$ 的概率
  >
  > ${\displaystyle R_{a}(s,s')}$ 是从 ${\displaystyle s}$ 由于动作 ${\displaystyle a}$ 转换为 ${\displaystyle s'}$ 状态后收到奖励
  >
  > 策略是最大化 ${\displaystyle E\left[\sum _{t=0}^{\infty }{\gamma ^{t}R_{a_{t}}(s_{t},s_{t+1})}\right ]},\;0\leq\gamma\leq1$ 

- **优化过程通过价值 $V(s)$ 更新与策略 $\pi(s)$ 迭代更新实现**

  > $$
  > \begin{aligned}
  > V(s) :=& \sum_{s'} P_{\pi(s)} (s,s') \left( R_{\pi(s)} (s,s') + \gamma V(s')\right)\\
  > \pi (s):=&\;\operatorname {argmax} a\left\{\sum _{s'}P_{a}(s,s')\left(R_{a}(s, s')+\gamma V(s')\right)\right\}
  > \end{aligned}
  > $$

