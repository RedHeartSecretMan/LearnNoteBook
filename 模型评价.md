# ***模型评价***

[TOC]



## **时间复杂度**

**用于评价模型运行效率高低，通常意味着模型运行速度**

- 计算复杂度使用浮点运算数 *FLOPs*  

- 模型并行性使用最大顺序操作数 *Minimum number of sequential operations* 和吞吐量 *Throughput (image/s)* 以及推理时间 *Inference time (bacth/ms)* 

  > 其中吞吐量与推理时间不仅仅与模型有关，还与硬件性能有关

#### ***FLOPs***

##### ***1.  Convolution***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7/%E5%8D%B7%E7%A7%AF.jpeg" alt="卷积" width="600" />
$$
FLOPs=(2\times C_{input}\cdot S_{filter_h}\cdot S_{filter_w}-1)^*\cdot C_{output}\cdot S_{input_h}\cdot S_{input_w}\\
\begin{aligned}\\
e.g.\quad 
&C_{input}=3\quad C_{output}=4\quad S_{filter_h}=S_{filter_w}=3\quad S_{input_h}=S_{input_w}=6\\
&FLOPs=(2\times3\times3^2-1)\times4\times6^2=7632
\end{aligned}
$$
**\* 卷积有偏置则不需要 *-1*** 

##### ***2.  Attention***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7/%E6%B3%A8%E6%84%8F%E5%8A%9B.png" alt="image-20220122113102360" width="600" />
$$
FLOPs=\begin{cases}
2D_kND_x\;+\;2D_kN^2\;+\;1\\

3D^2N\;+\;2DN^2\;+\;1\quad if\quad D_x=D_k=D_v=Our\,D_{model}=D
\end{cases}
$$

##### ***3.  Fully connected***

**假设全连接包括输入层隐含层输出层三层，输入层包含 *N* 批次 *D* 个神经元，隐含层包含 *N* 批次 *4D* 个神经元，输出层包含 *N* 批次 *D* 个神经元**
$$
\begin{align}\\
FLOPs\;&=\;D\cdot 4D\cdot N+4D\cdot D\cdot N\\
&=\;8D^2N

\end{align}
$$

## **空间复杂度**

**用于评价模型占用空间大小，通常意味着模型能否运行** 

- 参数量 *Parameters* 
- 数据位数 *Data bits* 

#### ***Parameters***

$$
Parameters=Volume(Tensor_{Wight})
$$

#### ***Data bits***

$$
Float32\quad or\quad Float64
$$

## **深度学习领域调研**

### ***0.  Attention Is All You Need***

**Per-layer complexity, minimum number of sequential operations for different layer types and maximum path length** 

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7/image-20220121125955712.png" alt="image-20220121125955712" width="1000" />

**In tabel *n* is the sequence length, *d* is the representation dimension, *k* is the kernelsize of convolutions and *r* the size of the neighborhood in restricted self-attention**

**首次提出完全基于注意力和全联接的 *Transformer* 架构的自然语言处理神经网络，值得注意最大路径长度 *(maximum path length)* $O(x)$ $x$ 越大代表在长距离依赖的结点传递信息时，信息交互越难，信息丢失越严重** 



### ***1.  Densely Connected Convolutional Networks***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7/image-20220120211612502.png" alt="image-20220120211612502" width="1000" />

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7/image-20220120204504919.png" alt="image-20220120204504919" width="1000" />

**具有 *BottleNeck* 结构的 *DenseNet- L*$(k=n)$ ，** ***L* 代表模型深度，即可学习的层数（卷积层，全连接层等）*$k$* 为输入的 *feature* 经过一个 *Dense Block* 中的一个 *Dense Layer* 后增加的特征通道数，经过一个 *Dense Block* 后，紧接着的 *Transition Layer* 后会将当前 *feature* 的特征通道数压缩一半** 

### ***2.  Deep Residual Learning for Image Recognition***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7/image-20220121203650386.png" alt="image-20220121203650386" width="1000" />

**其中 *FLOPs* 被误为 *MACs*，实际 *FLOPs* 应该是上述的两倍大小，*L-layer* 中 *L* 代表可学习的层数** 

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7/image-20220120205312977.png" alt="image-20220120205312977" width="600" />

**加入 *bottleneck* 结构后网络参数量明显下降，实现了超过 *1000* 层的网络** 

### ***3.  https://github.com/sovrasov/flops-counter.pytorch***

**通过调用外部库 *flops-counter* 计算的主流卷积模型的参数量和乘加操作数，并相应给出了 *Top1* 和 *Top5* 精度** 

|       Model        | Input Resolution | Params(M) | MACs(G) | Acc@1  | Acc@5  |
| :----------------: | :--------------: | :-------: | :-----: | :----: | :----: |
|   ***alexnet***    |     224x224      |   61.1    |  0.72   | 56.432 | 79.194 |
| ***densenet121***  |     224x224      |   7.98    |  2.88   | 74.646 | 92.136 |
| ***densenet161***  |     224x224      |   28.68   |  7.82   | 77.56  | 93.798 |
| ***densenet169***  |     224x224      |   14.15   |  3.42   | 76.026 | 92.992 |
| ***densenet201***  |     224x224      |   20.01   |  4.37   | 77.152 | 93.548 |
|    ***dpn107***    |     224x224      |   86.92   |  18.42  | 79.746 | 94.684 |
|    ***dpn131***    |     224x224      |   79.25   |  16.13  | 79.432 | 94.574 |
|    ***dpn68***     |     224x224      |   12.61   |  2.36   | 75.868 | 92.774 |
|    ***dpn68b***    |     224x224      |   12.61   |  2.36   | 77.034 | 93.59  |
|    ***dpn92***     |     224x224      |   37.67   |  6.56   |  79.4  | 94.62  |
|    ***dpn98***     |     224x224      |   61.57   |  11.76  | 79.224 | 94.488 |
| ***inceptionv3***  |     299x299      |   27.16   |  5.73   | 77.294 | 93.454 |
| ***inceptionv4***  |     299x299      |   42.68   |  12.31  | 80.062 | 94.926 |
|  ***resnet101***   |     224x224      |   44.55   |  7.85   | 77.438 | 93.672 |
|  ***resnet152***   |     224x224      |   60.19   |  11.58  | 78.428 | 94.11  |
|   ***resnet18***   |     224x224      |   11.69   |  1.82   | 70.142 | 89.274 |
|   ***resnet34***   |     224x224      |   21.8    |  3.68   | 73.554 | 91.456 |
|   ***resnet50***   |     224x224      |   25.56   |  4.12   | 76.002 | 92.98  |
| ***se_resnet101*** |     224x224      |   49.33   |  7.63   | 78.396 | 94.258 |
| ***se_resnet152*** |     224x224      |   66.82   |  11.37  | 78.658 | 94.374 |
| ***se_resnet50***  |     224x224      |   28.09   |   3.9   | 77.636 | 93.752 |
|    ***vgg11***     |     224x224      |  132.86   |  7.63   | 68.97  | 88.746 |
|    ***vgg13***     |     224x224      |  133.05   |  11.34  | 69.662 | 89.264 |
|    ***vgg16***     |     224x224      |  138.36   |  15.5   | 71.636 | 90.354 |
|    ***vgg19***     |     224x224      |  143.67   |  19.67  | 72.08  | 90.822 |

### ***4.  AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7/image-20220122152835106.png" alt="image-20220122152835106" width="600" />

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7/image-20220122154046936.png" alt="image-20220122154046936" width="600" />

***VIT* 完全基于注意力机制和全连接的视觉神经网络**



### ***5.  Swin Transformer: Hierarchical Vision Transformer using Shifted Windows***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7/image-20220122152002688.png" alt="image-20220122152002688" width="1000" />

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7/image-20220122153651890.png" alt="image-20220122153651890" width="600" />

***Swin* 完全基于具有滑动窗口的注意力机制和全连接的视觉神经网络**

## **散射成像领域的模型对比**

***以下的计算 Batch 统一设置为 2***

### ***1.  Deep speckle correlation: a deep learning approachtoward scalable imaging through scattering media***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7/image-20220122144343659.png" alt="image-20220122144343659" width="600" />

- ***Input Resolution* : $256\times 256$**

- ***Parameters* : $21.8505\times 10^6$** 
- ***FLOPs* : $0.0577\times 10^9$ ** 
- ***Throughput* :** $8.9\,image/s$
- ***Inference time* :** $223.2022\,batch/ms$

### ***2.  High-generalization deep sparse pattern reconstruction: feature extraction of speckles using self-attention armed convolutional neural networks***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7/image-20220122144525611.png" alt="image-20220122144525611" width="600" />

#### ***SA-CNN***

- ***Input Resolution* : $256\times 256$**
- ***Parameters* : $13.9231\times 10^6$** 
- ***FLOPs* : $17.4204\times 10^9$ ** 
- ***Throughput* :** $40.8\,image/s$
- ***Inference time* :** $49.0446\,batch/ms$

#### ***SA-CNN-Single\****

- ***Input Resolution* : $256\times 256$**
- ***Parameters* : $13.5972\times 10^6$** 
- ***FLOPs* : $8.9002\times 10^9$ ** 
- ***Throughput* :** $44.4\,image/s$
- ***Inference time* :** $45.0413\,batch/ms$

**\* 仅有中间一层注意力** 

### ***3.  Our SpT UNet***

<img src="Markdown%E5%9B%BE%E5%BA%8A/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7/SpT%20UNet.png" alt="Xnip2022-01-22_15-06-45" width="600" />

#### ***SpT UNet***

- ***Input Resolution* : $200\times 200\quad 224\times 224\quad 256\times 256$**
- ***Parameters* : $6.6184\times 10^6$** 
- ***FLOPs* : $19.3602\times 10^9\quad 24.2856\times 10^9\quad 31.7197\times 10^9$ **
- ***Throughput* :** $86.9\,image/s\quad 83.3\,image/s\quad 62.5\,image/s\quad$
- ***Inference time* :** $23.0214\,batch/ms\quad 24.0215\,batch/ms\quad 31.3427\,batch/ms$

#### ***SpT UNet-B\****

- ***Input Resolution* : $200\times 200\quad 224\times 224\quad 256\times 256$**
- ***Parameters* : $2.4179\times 10^6$** 
- ***FLOPs* : $8.2659\times 10^9\quad 16.2256\times 10^9\quad 21.2318\times 10^9$ **
- ***Throughput* :** $105.2\,image/s\quad 95.2\,image/s\quad 72.9\,image/s\quad$
- ***Inference time* :** $19.0217\,batch/ms\quad 21.0189\,batch/ms\quad 27.4584\,batch/ms$

**\* *puffed* 下采样和 *leaky* 上采样采用 *Bottleneck* 结构** 

