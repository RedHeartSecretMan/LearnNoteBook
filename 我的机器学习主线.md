# ***我的机器学习主线***

[toc]



## **前言**

**命题是逻辑的起点，命题中一类是通过无数的实验与经验验证，被广泛承认正确的数学公理与物理定律；另一类是基于这些公理与定律的逻辑推论即定理。公理与定律有适用范围，不可被证明，通过归纳法得出，通常是父命题，定理可以被证明，通过演绎法得出，通常是子命题，这些命题构建了机器学习**



## **背景**

**机器学习是什么？越是简单的概念越是难以定义。比如有人认为机器学习问题事实上是一个优化问题，有人认为是机器学习是一个编程概念，也有人认为现阶段的机器学习是统计推断。Tom Mitchell 认为每个机器学习都可以精准地定义为「任务T」、「训练过程E」和「模型表现P」，机器学习的过程则可以拆解为「为了实现任务T」，我们「通过训练过程E」，逐步「提高表现P」的一个过程**



### **思想实验**

**假如世界上还没有机器学习，我们希望机器能够复刻人类与生俱来的学习能力，我们应该如何从零开始实现它**

- **白盒思路**

  >  *把问题掰开了揉碎了，梳理清晰问题的内部结构，从根子上建构完整的理解*

- **黑盒思路**

  > *不纠结问题的内部结构，不要求完全弄懂问题的来龙去脉，只需通过它与周围其他事物间的关系确定它的边界，清晰的把握它的作用和意义* 

- **定义黑盒的学习**

  > *学习绝不是简易记忆而是基于记忆作出<font face="楷体" color=red> 预判 </font>，学习经历过的事情让我们对没有经历的事情作出准确的预判，学习的内容虽然都是已知，但是学习后的经验却可以推广到未知*



### **如何工作**

**通过演绎、归纳和演化实现预判**

- **演绎**

  > *演绎是绝对的理性。最典型演绎是数学，首先做好定义、公理和逻辑规则等前置条件，然后基于定义与公理通过逻辑规则进行推理，最后建立一个庞大知识体系，其中定义、公理和逻辑规则是已知推理建立的庞大知识体系是未知。演绎的学习方法要求我们相信绝对的理性，其背后是柏拉图视角下的世界观，认为存在一个永恒的一致的真理，通过理性我们可以拨云见雾掌握真理，基于演绎的符号主义是实现人工智能最早的流派，然而现阶段仅仅依靠预设的前置条件进行演绎却连自然语言处理任务也难以实现*

- **归纳**

  > *归纳是理性和经验的结合。现阶段实现自然语言处理任务的主流就是使用数学模型加数据的方法，数学模型就是理性，数据就是经验代表既定的事实，事实背后的规律需要寻找，可以利用抽象能力把数据的有效特征提取出来，通过理性建立特征之间的联系寻找背后的规律。归纳背后是承认这个世界不是绝对理性的而是有模糊的不确定的部分，但是相信这部分间具有规律。基于归纳的连接主义实现人工智能时广泛使用的神经网络就是定义数学模型并且提供数据从中总结规律*

  > *值得注意的是，通过演绎得出的结论通常是不会出错而通过归纳得出的结论则可能因为数据量不够导致得出的结论出错，直观上给人的感觉就是演绎更可靠，归纳不可靠，然而归纳不可靠却成为了学习过程中的优势，归纳内秉的模糊与不确定有助于解决自然语言处理问题，自然语言是人类社会发展过程中自然产生的语言，是人类智慧和文明的产物，人类并不是完全理性的动物，因此自然语言背后很难说有一个永恒的一致的真理* 

  > *当下机器学习的主流的方法通常是归纳，例如通过最小间隔的支持向量机与通过梯度下降的神经网络*

- **演化**

  > *演化是纯粹的经验。演化是没有方向的变化，可以是由简单到复杂的进化，也可以是由复杂到简单的退化。演化的主要机制是生物的可遗传变异，以及生物对环境的适应和物种间的竞争。基于演化的行为主义中典型学习算法有遗传算法，同样具有从已知到未知的过程，一代一代的变异与反馈就是已知，而下一代的表现就是未知*

  > *值得注意的是，演化不会主动的利用理性从已有的经验中总结规律，演化的核心是实践，吃一堑长一智通过不断的迭代将有效的经验一条条的记录到一个巨型列表中，这个过程没有抽象没有推理也不需要理性，但是仍然可以跨越从已知到未知的边界，仍然算是学习，理性是人类具有的高级思维，细菌与植物不具有但是仍然可以通过演化的方式进行学习*

- **演绎、归纳和演化分别对应符号主义、连接主义和行为主义三个流派的理念**



## **概率论**

**概率论研究随机变量的性质，比如已知随机变量满足正态分布，推断随机变量的期望与方差。通常使用数理统计的方法**

**支持向量机与神经网络的理论基础是大数定律和中心极限定理，大数定律研究一系列随机变量 $X_1,X_2,\cdots,X_n$  的样本均值 $\overline{X}_n = \frac{1}{n}\sum^n_{k=1}X_k$ 是否会收敛于总体均值即期望 $E(X_n)$；中心极限定理研究一系列随机变量 $\{X_n\}$ 不论服从何种分布，其均值 $\overline{X}_n$ 在随机变量的数量 $n$ 足够大时，$\overline{X}_n$ 总会趋于正态分布**



### **大数定律**

**大数定律分为强大数定律和弱大数定律，前者关于的独立性、异质性（同分布）和矩条件（均值与方差，在统计工作中描述集中趋势和离散程度的两个最重要的测度值）的约束条件强于后者。大数定律表明，随着样本数的增大，可以用样本均值来估计总体均值**

- **独立同分布**

  > *独立代表样本是独立的事件即实验中一个事件的发生不会影响到另一事件发生。例如 $A、B$ 是两个事件，满足 $P(A ∩ B) = P(AB) = P(A)P(B)$，则事件 $A$ 和 $B$ 相互独立。*
  >
  > *同分布意味所有样本中的属于相同的分布。分布即概率分布是事件概率的数学函数例如离散变量的概率质量函数 $p(x)$ 与连续变量的概率密度函数 $f(x)$ 或累积分布函数 $\Phi(x)$，根据「样本空间即观察到的随机现象的所有可能事件的集合」与「事件概率即样本空间的子集」使用数学语言描述随机现象*
  >
  > *在机器学习中通常假设数据独立同分布，得似然函数 $I(\Theta)$ 并取对数似然函数 $log(I(\Theta))$ 求极值，其中对数变换提高了效率因为计算机加法运算快于乘法，并且容易应用中心极限定理* 
  > $$
  > \begin{aligned}
  > &\mathrm{argmax}\;I(\Theta)=P(X_1,X_2,X_3,....X_n|\Theta)=P(X_1|\Theta)\ast P(X_2|\Theta)\ast ...\ast P(X_n|\Theta)\\
  > &\mathrm{argmax}\;log(I(\Theta))=log(P(X_1|\Theta)) + log(P(X_2|\Theta)) + ... + log(P(X_n|\Theta))
  > \end{aligned}
  > $$

- **均值与方差**

  > *均值是随机变量的算术平均值，方差是随机变量与其总体均值或样本均值的偏差的平方的算术平均值* 
  >
  > *离散的随机变量 $X$，概率质量函数为 $p(x)$​*
  > $$
  > \begin{aligned}
  > D(X)=&\sum_{i=1}^n p(x_i)\cdot(x_i - \sum_{i=1}^n (p(x_i)\cdot x_i))^2\\
  > =&E\{[X-E(X)]^2\}\\
  > =&E(X^2)-E(2XE(X))+E(X)^2\\
  > =&E(X^2)-2E(X)E(X)+E(X)^2\\
  > =&E(X^2)-E(X)^2
  > \end{aligned}
  > $$
  > *连续的随机变量 $X$，定义域为 $(a,b)$，概率密度函数为 $f(x)$*
  > $$
  > \begin{aligned}
  > D(X)=&\int^b_a(x-\mu)^2f(x)\,dx\\
  > =&\int^b_ax^2f(x)\,dx-\int^b_a2x\mu f(x)\,dx-\int^b_a\mu^2 f(x)\,dx\\
  > =&\int^b_ax^2f(x)\,dx-2\mu\int^b_ax f(x)\,dx-\mu^2\cdot1\\
  > =&\int^b_ax^2f(x)\,dx-2\mu\cdot\mu-\mu^2\\
  > =&\int^b_ax^2f(x)\,dx-\mu^2\\
  > =&E(X^2)-E(X)^2
  > \end{aligned}
  > $$
  > *随机变量的方差是随机变量的平方的期望与随机变量的期望的平方之和，随机变量的均值与方差不一定都存在* 
  > $$
  > \pmb{\text{存在均值}\not\Rightarrow\text{存在方差}}\\
  > \pmb{\text{存在方差}\Rightarrow\text{存在均值}}
  > $$
  > *如果多个随机变量不确定是否独立，则误差分析需考虑多个随机变量的总体误差即协方差* 
  > $$
  > \begin{aligned}
  > Cov(X,Y)=&E[(X-\mu)(Y-v)]\\
  > =&E(X\cdot Y)-\mu v
  > \end{aligned}
  > $$
  > *值得注意的是随机变量独立是协方差 $Cov(X,Y)=E(X)\cdot E(Y)-\mu v=0$ 的充分不必要条件，随机变量独立需要保证随机变量间既没有线性相关也没有非线性相关，随机变量的协方差为零仅代表没有线性相关性即 $\eta ={\frac {Cov(X,Y)}{\sqrt {D(X)\cdot D(Y)}}}=0$，即一系列随机变量 $X1,\cdots, Xn$ 有*
  > $$
  > \begin{aligned}
  > D\left(\sum _{i=1}^{n}X_{i}\right)=\sum _{i=1}^{n}D(X_{i})+2\sum _{i,j\,:\,i<j}Cov (X_{i},X_{j})
  > \end{aligned}
  > $$
  > *少数分布的随机变量 $X$ 没有均值也没有方差，如柯西分布概率密度函数* 
  > $$
  > \begin{aligned}
  > f(x;x_0,\gamma)=&
  > \frac{1}{\pi\gamma \left[1 + \left(\frac{x-x_0}{\gamma}\right)^2\right]}\\
  > =&{1 \over \pi }\left[{\gamma \over (x-x_0)^{2}+\gamma ^{2}}\right]\\
  > \\
  > &\text{when}\;x_0=0\;\text{and}\;\gamma=1\;\text{是标准柯西分布}\\
  > \\
  > f(x;0,\gamma)=&
  > \frac{1}{\pi\left(1 + x^2\right)}
  > \end{aligned}
  > $$
  > *$X_0$ 定义分布峰值位置，$\gamma>0$ 定义分布半峰位置*

- **总体均值与样本均值以及总体方差与样本方差**

  > *样本均值和方差是总体均值和方差的估计量* 

  > *总体均值是客观存在的具有某一特性的一类事物的全体 $N$ 的加权平均即期望 $\mu$ 是真值*
  >
  > *样本均值是从总体中抽出 $n$ 份个体的加权平均 $X_n$ 是无偏估计量*

  > *全体随机变量为 $X_1,X_2,\cdots,X_N$ 则总体方差为 $D(X_N)=\sigma^2 = \frac{1}{N}\sum^N_{k=1}(X_k-\mu)^2$ 是真值*
  >
  > *全体随机变量的抽样为 $X_1,X_2,\cdots,X_n$ 的样本方差为 $D(X_n)=S^2 = \frac{1}{n-1}\sum^n_{k=1}(X_k-\overline{X}_n)^2$ 是无偏估计量而更直观的 $\frac{1}{n}\sum^n_{k=1}(X_k-\overline{X}_n)^2$ 是有偏估计量*
  > $$
  > \begin{aligned}
  > S^2=&\frac{1}{n-1}\sum^n_{k=1}(X_k-\overline{X}_n)^2\\
  > =&\frac{1}{n-1}\sum^n_{k=1}\left[(X_k-\mu)-(\overline{X}_n-\mu)\right]^2\\
  > =&\frac{1}{n-1}\left[\sum^n_{k=1}(X_k-\mu)^2-2\sum^n_{k=1}(X_k-\mu)(\overline{X}_n-\mu)+\sum^n_{k=1}(\overline{X}_n-\mu)^2\right]\\
  > =&\frac{1}{n-1}\left[\sum^n_{k=1}(X_k-\mu)^2-2(\overline{X}_n-\mu)\cdot\sum^n_{k=1}(X_k-\mu)+\sum^n_{k=1}(\overline{X}_n-\mu)^2\right]\\
  > =&\frac{1}{n-1}\left[\sum^n_{k=1}(X_k-\mu)^2-2(\overline{X}_n-\mu)\cdot\sum^n_{k=1}(X_k-\mu)+n\cdot(\overline{X}_n-\mu)^2\right]\\
  > =&\frac{1}{n-1}\left[\sum^n_{k=1}(X_k-\mu)^2-2(\overline{X}_n-\mu)\cdot n(\overline{X}_n-\mu)+n\cdot(\overline{X}_n-\mu)^2\right]\\
  > =&\frac{1}{n-1}\left[\sum^n_{k=1}(X_k-\mu)^2-n\cdot(\overline{X}_n-\mu)^2\right]\\
  > =&\frac{1}{n-1}\left[\sum^n_{k=1}\sigma_{X_k}^2-n\sigma_{\overline{X}_n}^2\right]\\
  > =&\frac{1}{n-1}\left[n\sigma^2-n(\frac{\sigma^2}{n})\right]\\
  > =&\sigma^2\\
  > \\
  > &\begin{aligned}
  > &\text{其中}\;\sigma_{X_k}^2\;\text{抽样数}\;n\;\text{足够大可以认为等价}\;\sigma^2\\
  > &\text{其中}\;\sigma_{X_n}^2\;\text{抽样的随机变量独立同分布则}\;\sum _{i,j\,:\,i<j}Cov (X_{i},X_{j})=0\;\text{即可以认为方差等价}\;\sigma^2\\
  > \\
  > &\begin{aligned}
  > \sigma_{\overline{X}_n}^2
  > =&\left[\left(\frac{X_1+X_2+\cdots+X_n}{n}\right)-\mu\right]^2\\
  > =&\frac{\left[(X_1-\mu)+(X_2-\mu)+\cdots+(X_n-\mu)\right]^2}{n^2}\\
  > =&\frac{n\cdot(X_1-\mu)^2}{n^2}\\
  > =&\frac{\sigma^2}{n}
  > &\end{aligned}
  > &\end{aligned}
  > \end{aligned}
  > $$
  > 

- **强大数定律**

  > *强大数定律描述样本均值 <font face="楷体" color=red> 几乎确定收敛 </font> 于总体均值* 
  > $$
  > \begin{aligned}
  > &\forall\epsilon>0,\;P\{\lim_{n\rightarrow\infty}{\mid\overline{X}_n-E(X_n)\mid<\epsilon}\}=1\\
  > &\forall\epsilon>0,\;\exists N\in {\Bbb{N}}^+,\;\text{When}\;n>N,\;P\{\mid\overline{X}_n-E(X_n)\mid<\epsilon\}=1\\
  > &\overline{X}_n\stackrel{\mathrm{a.s.}}{\longrightarrow}E(X_n)
  > \end{aligned}
  > $$
  > *也就是说最终样本均值收敛于总体均值的概率为 $1$，即样本均值函数的变量样本数超过界限后样本均值函数只存在有限跳跃间断点偏离总体均值超过 $\epsilon$* 

- **弱大数定律**

  > *弱大数定律描述样本均值 <font face="楷体" color=red> 依概率收敛 </font> 于总体均值* 
  > $$
  > \begin{aligned}
  > &\forall\epsilon>0,\;\lim_{n\rightarrow\infty}P\{\mid\overline{X}_n-E(X_n)\mid<\epsilon\}=1\\
  > &\forall\epsilon>0,\;\exists \delta>0,\;\exists N\in {\Bbb{N}}^+,\;\text{When}\;n>N,\;\mid P\{\mid\overline{X}_n-E(X_n)\mid<\epsilon\}-1\mid <\delta\\
  > &\overline{X}_n\stackrel{\mathrm{P}}{\longrightarrow}E(X_n)
  > \end{aligned}
  > $$
  > *也就是说最终样本均值收敛于总体均值的概率接近为 $1$，即样本均值函数的变量样本数超过界限后样本均值函数偏离总体均值超过 $\epsilon$ 的概率小于 $\delta$，意味着可以有无限的偏离点*



**大数定律衍生了诸多定理，在时间脉络上首先是基于弱大数定律的「伯努利大数定理」、「切比雪夫大数定理」和「辛钦大数定理」其次是基于强大数定律的「波莱尔强大数定理」与「柯尔莫哥洛夫强大数定理」**

- **伯努利大数定理**

  > *设 $X_1,X_2,\cdots,X_n$ 是一系列独立同分布的事件，存在 $P(X_k=1)=p$ 与 $P(X_k=0)=1-p$ 其中 $k=1,2,\cdots,n$ 以及概率 $p\in[0,1]$，若前 $n$ 项和 $S_n=\sum^n_{k=1}X_k$，则任意的正数 $\epsilon$ 满足公式 $(1)$*
  > $$
  > \tag{1}
  > \lim_{n\rightarrow\infty}P\left\{\mid\frac{S_n}{n}-p\mid<\epsilon\right\}=1
  > $$
  > *事件的频率 $\frac{S_n}{n}$ <font face="楷体" color=red> 依概率收敛 </font> 于概率 $p$*
  >
  > *约束条件为事件是独立的伯努利分布，均值存在且相同为 $p$ 和方差存在且相同为 $p(1-p)$*
  >
  > *切比雪夫不等式表明，任意 $\epsilon>0$ 有随机变量 $X$ 使得 $P\left\{\mid X- E(X)\mid\geq\epsilon\right\}\leq\frac{D(X)}{\epsilon^2}$* 
  > $$
  > \begin{aligned}
  > P\left\{\mid X- E(X)\mid\geq\epsilon\right\}
  > =&\int_{\mid x-\mu\mid\geq\epsilon}f(x)\,dx\\
  > \leq&\int_{\mid x-\mu\mid\geq\epsilon}\left(\frac{\mid x-\mu\mid}{\epsilon}\right)^2\cdot f(x)\,dx\\
  > \leq&\int_{-\infty}^{+\infty}\left(\frac{\mid x-\mu\mid}{\epsilon}\right)^2\cdot f(x)\,dx\\
  > =&\frac{D(X)}{\epsilon^2}
  > \end{aligned}
  > $$
  > *证明式 $(1)$ 等价形式 $\lim{n\rightarrow\infty},\;P\{\mid \frac{S_n}{n}- p\mid\geq\epsilon\}=0$*
  > $$
  > \begin{aligned}
  > &\because\quad\lim_{n\rightarrow\infty}P\{\mid \frac{S_n}{n}-p\mid\geq\epsilon\}
  > \leq \lim_{n\rightarrow\infty}\frac{p(1-p)}{n\epsilon^2}=0\\
  > &\because\quad\ P\{\mid \frac{S_n}{n}-p\mid\geq\epsilon\}\geq0\\
  > &\therefore\quad\ 0\leq\lim_{n\rightarrow\infty}P\{\mid \frac{S_n}{n}-p\mid\geq\epsilon\}\leq0\\
  > &\therefore\quad\ \lim_{n\rightarrow\infty}P\{\mid \frac{S_n}{n}-p\mid\geq\epsilon\}=0
  > \end{aligned}
  > $$

- **切比雪夫大数定理**

  > *设 $X_1,X_2,\cdots,X_n$ 是一系列相互独立的随机变量，存在期望 $E(X_k)=\mu_k<M_1$ 与方差 $D(X_k)=\sigma_k^2=M_2$ 其中 $M_1$ 与 $M_2$ 是常数，则任意的正数 $\epsilon$ 满足公式 $(2)$*
  > $$
  > \tag{2}
  > \lim_{n\rightarrow\infty}P\left\{\mid\frac{1}{n}\sum^n_{k=1}X_k-\frac{1}{n}\sum^n_{k=1}\mu_k\mid<\epsilon\right\}=1
  > $$
  > *随机变量的均值 $\overline{X}_n$ <font face="楷体" color=red> 依概率收敛 </font> 于 $E(\mu_k)$*
  >
  > *约束条件为随机变量相互独立，随机变量的均值存在不一定相同，随机变量的方差存在且相同* 

- **辛钦大数定理**

  > *设 $X_1,X_2,\cdots,X_n$ 是一系列独立同分布的随机变量，存在期望 $E(X_k)=\mu$，则任意的正数 $\epsilon$ 满足公式 $(3)$*
  > $$
  > \tag{3}
  > \lim_{n\rightarrow\infty}P\left\{\mid\frac{1}{n}\sum^n_{k=1}X_k-\mu\mid<\epsilon\right\}=1
  > $$
  > *随机变量的均值 $\overline{X}_n$ <font face="楷体" color=red> 依概率收敛 </font> 于 $\mu$*
  >
  > *约束条件为随机变量相互独立，随机变量的均值存且相同，随机变量的方差不要求存在* 

- **波莱尔强大数定理**

  > *设 $X_1,X_2,\cdots,X_n$ 是一系列独立同分布的随机变量，存在 $P(X_k=1)=p$ 与 $P(X_k=0)=1-p$ 其中 $k=1,2,\cdots,n$ 以及概率 $p\in(0,1)$，若前 $n$ 项和 $S_n=\sum^n_{k=1}X_k$，则任意的正数 $\epsilon$ 满足公式 $(4)$*
  > $$
  > \tag{4}
  > P\left\{\lim_{n\rightarrow\infty}\mid\frac{1}{n}\sum^n_{k=1}X_k-\mu\mid<\epsilon\right\}=1
  > $$
  > *随机变量的均值 $\frac{S_n}{n}$ <font face="楷体" color=red> 几乎必然收敛 </font> 于 $p$*
  >
  > *约束条件与伯努利大数定理相同* 

- **柯尔莫哥洛夫强大数定理**

  > 1. *设 $X_1,X_2,\cdots,X_n$ 是一系列独立同分布的随机变量，存在期望 $E(X_n)=\mu$，则任意的正数 $\epsilon$ 满足公式 $(5)$*
  >
  > $$
  > \tag{5}
  > P\left\{\lim_{n\rightarrow\infty}\mid\frac{1}{n}\sum^n_{k=1}X_k-\mu\mid<\epsilon\right\}=1
  > $$
  >
  > 2. *设 $X_1,X_2,\cdots,X_n$ 是一系列相互独立的随机变量，存在期望 $E(X_k)=\mu_k$ 与方差 $D(X_k)=\sigma_k^2$ 其中 $\sigma_k^2$ 有限，则任意的正数 $\epsilon$ 满足公式 $(6)$*
  >
  > $$
  > \tag{6}
  > P\left\{\lim_{n\rightarrow\infty}\mid\frac{1}{n}\sum^n_{k=1}X_k-\frac{1}{n}\sum^n_{k=1}\mu_k\mid<\epsilon\right\}=1
  > $$
  >
  > *随机变量的均值 $\overline{X}_n$ <font face="楷体" color=red> 几乎必然收敛 </font> 于 $\mu$*
  >
  > 1. *约束条件为随机变量相互独立，随机变量的均值存且相同，随机变量的方差不要求存在*
  > 2. *约束条件为随机变量相互独立，随机变量的均值与方差存在且不一定相同*



### **中心极限定理**

**中心极限定理表明只要样本够多不论样本属于何种分布，样本均值会服从以总体均值为 $\mu$，总体方差为 $\sigma^2$ 的正态分布。假设全部随机变量是 $X_1,X_2,\cdots,X_n,X_{n+1},\cdots,X_{N}$，进行 $N-n$ 组抽样，每组抽样数为 $n$，最终得到 $\{\;\{X_1,X_2,\cdots,X_n\},\{X_2,X_3,\cdots,X_{n+1}\},\cdots,\{X_{(N-n)+1},X_{(N-n)+2},\cdots,X_N\}\;\}$ 分别计算每组的样本均值都将满足正态分布 $f(x)={{1}\over{\sigma {\sqrt {2\pi }}}}e^{-{\frac {1}{2}}\left({\frac {x -\mu }{\sigma }}\right)^{2}}$ 其中 $x= \overline{X}_1,\overline{X}_2,\cdots,\overline{X}_{N-n}$**

- **概率分布的特征函数**

  > *统计学通过矩系统刻画随机变量的概率分布，常见的矩有一阶矩 $M_1$ 即期望 $\mu=E(\color{red}{X}\color{gray}{)}$、二阶矩 $M_2$ 即方差 $\sigma^2=E(\color{red}{(X-\mu)^2}\color{gray}{)}$、三阶矩 $M_3$ 即偏态 $\tilde{\mu}_3=E((\frac{\color{red}{X-\mu}}{\sigma})\color{red}{^{3}}\color{gray}{)}$ 度量概率分布的不对称性和四阶矩 $M_4$ 即峰态 $\kappa=E((\frac{\color{red}{X-\mu}}{\sigma})\color{red}{^4}\color{gray}{)}$ 度量概率分布的陡峭程度等。随机变量的特征函数包含了它的全部的矩从另一个角度定义了概率分布*
  >
  > *连续的随机变量 $X$ 的概率密度函数为 $f(x)$，特征函数为 $\varphi_X(t)=E(e^{itX})=\int_{-\infty}^{+\infty}e^{itx}f(x)\,dx\;,t\in\Bbb{R}$ 是 $t$ 的复变函数，Herbert Wilf 比喻它是一个上面挂一串数字展示的晒衣绳，从泰勒级数角度看 $e^{itX}$ 等价 $1+\frac{itX}{1}-\frac{t^2X^2}{2!}+\cdots+\frac{(it)^nX^n}{n!}$ 也即 $E(e^{itX})=1+\frac{itE(X)}{1}-\frac{t^2E(X^2)}{2!}+\cdots+\frac{(it)^nE(X^n)}{n!}$ 其中包含各阶矩，对 $t$ 求导得 $k$ 阶矩 $\varphi_X^{(k)}(0)=i^kE(X^k)$；从傅立叶变换的角度看 $F_X(t)=\int_{-\infty}^{+\infty}e^{-itx}f(x)\,dx$ 概率密度函数的特征函数与其傅立叶变换共轭 $\varphi_X(t)=\overline{F_X(t)}$，根据欧拉公式 $e^{ix}=cos(x)+isin(x)$ 可知特征函数实质上就是等价于变换到了傅立叶坐标系*
  >
  > *特征函数虽然不如概率密度函数直观反应各种可能性，但是无法得知概率密度函数时可以先计算出各阶矩然后估计特证函数并通过反变换求解概率密度函数从而描述随机变量的概率分布*
  >
  > *对于独立随机变量 $Y_1=aX+b$ 与 $Y_2=X_1+X_2$ 的特征函数具有公式 $(7)$ 与 $(8)$ 的性质*
  > $$
  > \tag{7}
  > \begin{aligned}
  > \varphi_{Y_1}(t)=&\varphi_{aX+b}(t)\\
  > =&\int_{-\infty}^{+\infty}e^{it(ax+b)}\cdot f(x)\,dx\\
  > =&e^{ibt}\cdot\int_{-\infty}^{+\infty}e^{i(at)x}\cdot f(x)\,d(x)\\
  > =&e^{ibt}\cdot \varphi_X(at)
  > \end{aligned}
  > $$
  >
  > $$
  > \tag{8}
  > \begin{aligned}
  > \varphi_{Y_2}(t)=&\varphi_{X_1+X_2}(t)\\
  > =&\iint_{-\infty}^{+\infty}e^{it(x_1+x_2)}\cdot f_1(x_1)\cdot f_2(x_2)\,dx_1dx_2\\
  > =&\int_{-\infty}^{+\infty}e^{itx_1}\cdot f(x_1)\,dx_1\cdot\int_{-\infty}^{+\infty}e^{itx_2}\cdot f_2(x_2)\,dx_2\\
  > =&\varphi_{X_1}(t)\cdot \varphi_{X_2}(t)
  > \end{aligned}
  > $$

- **正态分布的特征函数**

  > *若随机变量 $X～\mathcal{N}(0,1)$ 其概率密度函数为 $f(x)=\frac{1}{\sqrt{2\pi}}e^{{\frac{-x^{2}}{2}}}$ 则有特征函数*
  > $$
  > \begin{aligned}
  > \varphi_X(t)=&\int_{-\infty}^{+\infty}e^{itx}\cdot\frac{1}{\sqrt{2\pi}}e^{{\frac{-x^{2}}{2}}}\,dx\\
  > =&e^{\frac{-t^2}{2}}\cdot\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{{\frac{-(x-it)^{2}}{2}}}\,d(x-it)\\
  > =&e^{\frac{-t^2}{2}}
  > \end{aligned}
  > $$

- **林德贝格-勒维中心极限定理**

  > *设 $X_1,X_2,\cdots,X_n$ 是一系列独立同分布的随机变量，前 $n$ 项和 $S_n=\sum^n_{k=1}X_k$ 均值为 $\overline{X}_n=\frac{1}{n}S_n$ 期望为 $E(X_n)=\mu$ 与方差 $D(X_n)=\sigma^2<\infty$，当 ${\textstyle n}$ 趋近无穷大，有公式 $(9)$*
  > $$
  > \tag{9}
  > \begin{aligned}
  > &\begin{aligned}
  > \lim _{n\to \infty } P \left\{{\sqrt {n}}({\overline {X}}_{n}-\mu )\leq z\right\} &=\lim _{n\to \infty } P \left\{{\frac {{\sqrt {n}}({\overline {X}}_{n}-\mu )}{\sigma }}\leq {\frac {z}{\sigma }}\right\}
  > \\&=\Phi(\frac{z}{\sigma})
  > ={\frac {1}{\sqrt {2\pi }}}\int_{-\infty}^{z\over\sigma}e^{-t^{2}/2}\,dt\\
  > \end{aligned}
  > \\&{\sqrt {n}}({\overline {X}}_{n}-\mu )\stackrel{\mathrm{D}}{\longrightarrow}{\mathcal {N}}(0,\sigma ^{2})
  > \end{aligned}
  > $$
  > *随机变量 ${\textstyle {\sqrt {n}}({\overline{X}}_{n}-\mu )}$ <font face="楷体" color=red> 依收分布收敛 </font> 于正态分布 ${\textstyle {\mathcal {N}}(0,\sigma ^{2})}$*
  >
  > *均匀收敛且 ${\displaystyle \lim _{n\to \infty }\;\sup _{z\in \mathbb {R} }\;\left|P \left[{\sqrt {n}}({ \overline{X}}_{n}-\mu )\leq z\right]-\Phi \left({\frac {z}{\sigma }}\right)\right|=0}$* 
  >
  > *证明 $\lim{n\rightarrow\infty},\;\frac{\sqrt{n}({\overline{X}}_{n}-\mu)}{\sigma}\stackrel{\mathrm{D}}{\longrightarrow}{\mathcal{N}}(0,1)$*
  > $$
  > \begin{aligned}
  > &\because\;
  > \text{式}(7),(8)\\
  > &\therefore\;
  > \varphi_{S_n}(t)=\left[\varphi_{x}(t)\right]^n,\;\varphi_{\overline{X}_n}(t)=\varphi_{S_n}(\frac{t}{n})=\left[\varphi_{x}(\frac{t}{n})\right]^n\\
  > &\because\;
  > Y=\frac{\sqrt{n}({\overline{X}}_{n}-\mu)}{\sigma}=\frac{\sqrt{n}}{\sigma}{\overline{X}}_{n}-\frac{\sqrt{n}}{\sigma}{\mu}\\
  > &\therefore\;
  > \varphi_{Y}(t)=e^{i(-\frac{\sqrt{n}}{\sigma}\mu)t}\cdot \varphi_{\overline{X}_n}(\frac{\sqrt{n}}{\sigma}t)=e^{i(-\frac{\sqrt{n}}{\sigma}\mu)t}\cdot \left[\varphi_{X}(\frac{t}{\sigma\sqrt{n}})\right]^n
  > \\
  > &\therefore
  > \\
  > &\begin{aligned}
  > \quad\;\; ln\left[\varphi_{Y}(t)\right]
  > =&\;ln\left\{e^{i(-\frac{\sqrt{n}}{\sigma}\mu)t}\cdot \left[\varphi_{X}(\frac{t}{\sigma\sqrt{n}})\right]^n\right\}\\
  > =&\;-i\frac{\sqrt{n}}{\sigma}\mu t+n\cdot ln\left[\varphi_{X}(\frac{t}{\sigma\sqrt{n}})\right]\\
  > =&\;n\left\{-i\mu\frac{t}{\sigma\sqrt{n}}+ln\left[\varphi_{X}(\frac{t}{\sigma\sqrt{n}})\right]\right\}
  > \\
  > \end{aligned}
  > \\
  > &\because\;
  > p=\frac{t}{\sigma\sqrt{n}},\;\lim_{n\rightarrow\infty}p=0,\;\lim_{n\rightarrow\infty}\varphi_{X}(p)=\varphi_{X}(0)
  > \\
  > &\because
  > \\
  > &\begin{aligned}
  > \quad\;\;
  > \varphi_{X}(0)=&\int_{-\infty}^{+\infty}f(x)\,dx=1\\
  > \varphi^\prime_{X}(0)=&\int_{-\infty}^{+\infty}ixf(x)\,dx=i\mu\\
  > \varphi^{\prime\prime}_{X}(0)=&\int_{-\infty}^{+\infty}-x^2f(x)\,dx=-\mu^2-\sigma^2\\
  > \end{aligned}
  > \\
  > &\therefore
  > \\
  > &\begin{aligned}
  > \quad\;\;
  > \lim_{n\rightarrow\infty}ln[\varphi_{Y}(t)]
  > =&\lim_{n\rightarrow\infty}n\left\{-i\mu\frac{t}{\sigma\sqrt{n}}+ln\left[\varphi_{X}(\frac{t}{\sigma\sqrt{n}})\right]\right\}\quad(\text{e.g.}\;\;0\cdot\infty)\\
  > =&\frac{t^2}{\sigma^2}\cdot\lim_{p\rightarrow0}\frac{-i\mu p+ln\left[\varphi_{X}(p)\right]}{p^2}\\
  > =&\frac{t^2}{\sigma^2}\cdot\lim_{p\rightarrow0}\frac{-i\mu+\left[\varphi_{X}(p)\right]^{-1}\cdot\varphi^{\prime}_{X}(p)}{2p}\\
  > =&\frac{t^2}{\sigma^2}\cdot\lim_{p\rightarrow0}\frac{\varphi^{\prime\prime}_{X}(p)\cdot\varphi_{X}(p)-\varphi^{\prime}_{X}(p)\cdot\varphi^{\prime}_{X}(p)}{2[\varphi_{X}(p)]^2}\\
  > =&\frac{t^2}{\sigma^2}\cdot\frac{\varphi^{\prime\prime}_{X}(0)\cdot\varphi_{X}(0)-[\varphi^{\prime}_{X}(0)]^2}{2[\varphi_{X}(0)]^2}\\
  > =&\frac{t^2}{\sigma^2}\cdot\frac{(-\mu^2-\sigma^2)\cdot1-(i\mu)^2}{2\cdot1}\\
  > =&-\frac{t^2}{2}
  > \end{aligned}
  > \end{aligned}
  > $$
  > *故 $\lim{n\rightarrow\infty},\;\varphi_Y(t)=e^{-\frac{t^2}{2}}$ 得证*

- **李雅普诺夫中心极限定理**

  > 设 $X_{1},\cdots ,X_{n}$ 是一系列独立随机变量，每个变量都有有限的期望 $\mu_k$ 和方差 $\sigma _{k}^{2}$ 定义 $S_n^2=\sum^{n}_{k=1}\sigma_k^2$，有公式 $(10)$
  > $$
  > \tag{10}
  > \begin{aligned}
  > &\exists\delta>0,\;\lim_{n\rightarrow\infty}\frac{1}{S_n^{2+\delta}}\cdot\sum_{k=1}^{n}E\left[\mid X_k-\mu_k\mid^{2+\delta}\right]=0\\
  > &{\frac{1}{S_n}}\sum_{k=1}^{n}({X}_{k}-\mu_k)\stackrel{\mathrm{D}}{\longrightarrow}{\mathcal {N}}(0,1)
  > \end{aligned}
  > $$
  > *随机变量的线性和 ${\frac{1}{S_n}}\sum_{k=1}^{n}({X}_{k}-\mu_k)$ <font face="楷体" color=red> 依收分布收敛 </font> 于正态分布 ${\textstyle {\mathcal {N}}(0,\sigma ^{2})}$*
  >
  > *通常使用检验条件 $\delta=1$*



### **连续分布**

#### **正态分布**

**正态分布 *(Normal Distribution)* 也称常态分布或正常分布，其概率密度函数为 $f(x)={\frac {1}{\sigma {\sqrt {2\pi }}}}e^{-{\frac {1}{2}}\left({\frac {x -\mu }{\sigma }}\right)^{2}}$，标准正态分布即高斯分布为 $\mu=1,\;\sigma^2=0,\;f(x)=\frac{1}{\sqrt{2\pi}}e^{{\frac{-x^{2}}{2}}}$ 其累积分布函数为 $\Phi (x)={\frac {1}{\sqrt {2\pi }}}\int _{-\infty }^{x}e^{-t^{2}/2}\,dt$ 相应的泰勒近似为 $\Phi (x)={\frac {1}{2}}+{\frac {1}{\sqrt {2\pi }}}\sum _{k=0}^{n}{ \frac {\left(-1\right)^{k}x^{\left(2k+1\right)}}{2^{k}k!\left(2k+1\right)}}$**

- **高斯推导**

  > *设 $x_{1},\cdots ,x_{n}$ 是真值 $x$ 的一系列独立观测值 $\bar{x}=\frac{1}{n}\sum^{n}_{k}x_k$，误差为 $\theta,\;\theta_k=x_k-x,\;k=1,2,\cdots,n$  其概率密度函数为 $f(\theta)$，似然函数为 $L(\theta)=\prod_{k=1}^{n}f(\theta_k)$，假设 $x=\bar{x}$ 并进行极大似然估计*
  > $$
  > \begin{aligned}
  > \because
  > \\&\begin{aligned}
  > L(\theta)=\prod_{k=1}^{n}f(\theta_k)
  > =&\prod_{k=1}^{n}f(x_k-x)\\
  > \frac{d}{dx}ln\left[\prod_{k=1}^{n}f(\theta_k)\right]
  > =&\frac{d}{dx}\left\{\sum_{k=1}^{n}ln[f(x_k-x)]\right\}\\
  > =&-\sum_{k=1}^{n}\frac{f^\prime(x_k-x)}{f(x_k-x)}
  > \end{aligned}\\
  > \therefore
  > \\&\sum_{k=1}^{n}\frac{f^\prime(x_k-\bar{x})}{f(x_k-\bar{x})}
  > =0\\
  > \because
  > \\&\theta_k=x_k-\bar{x}=t,\;\frac{f^\prime(t)}{f(t)}=g(t)\\
  > \therefore
  > \\&G(\theta)=\sum_{k=1}^{n}g(x_k-\bar{x})=0
  > \\&G^\prime(\theta)=\sum_{k=1}^{n}\frac{\partial[g(x_k-\bar{x})]}{\partial x_k}=0\\
  > \therefore
  > \\&\begin{aligned}
  > &g^\prime(x_1-\bar{x})(1-\frac{1}{n})+g^\prime(x_2-\bar{x})(-\frac{1}{n})+\cdots++g^\prime(x_n-\bar{x})(-\frac{1}{n})=0\\
  > &g^\prime(x_1-\bar{x})(-\frac{1}{n})+g^\prime(x_2-\bar{x})(1-\frac{1}{n})+\cdots++g^\prime(x_n-\bar{x})(-\frac{1}{n})=0\\
  > &\vdots\\
  > &g^\prime(x_1-\bar{x})(1-\frac{1}{n})+g^\prime(x_2-\bar{x})(-\frac{1}{n})+\cdots++g^\prime(x_n-\bar{x})(-\frac{1}{n})=0\\
  > \end{aligned}\\
  > \because
  > \\&\begin{bmatrix}   
  > 1-\frac{1}{n} & -\frac{1}{n} & \cdots & -\frac{1}{n} \\   
  > -\frac{1}{n} & 1-\frac{1}{n} & \cdots & -\frac{1}{n} \\  
  > \vdots & \vdots & \ddots & -\frac{1}{n} \\
  > -\frac{1}{n} & -\frac{1}{n} & \cdots & 1-\frac{1}{n} \\
  > \end{bmatrix}\begin{bmatrix}   
  > g^\prime(x_1-\bar{x})\\   
  > g^\prime(x_2-\bar{x})\\   
  > \vdots\\
  > g^\prime(x_n-\bar{x})\\
  > \end{bmatrix}=\begin{bmatrix}   
  > 0\\   
  > 0\\   
  > \vdots\\
  > 0\\
  > \end{bmatrix}\\
  > \therefore
  > \\&g^\prime(x_k-\bar{x})=C,\;k=1,2,\cdots,n\\
  > &\sum_{k=1}^{n}g(x_k-\bar{x})=\sum_{k=1}^{n}C(x_k-x_n)+nb=0+nb=0,\;b=0\\
  > \therefore
  > \\&g(t)=\frac{f^\prime(t)}{f(t)}=Ct\\
  > &f(t)=Ae^{\frac{1}{2}Ct^2}\\
  > &\lim_{n\rightarrow\infty}\sum^{n}_{k=1}f(x_k-\bar{x})=\sum^{\infty}_{k=1}f(x_k-x)\\
  > \because
  > \\&\Phi(\theta)=\sum^{\infty}_{k=1}f(x_k-\bar{x})=\int_{-\infty}^{+\infty}f(\theta)\,d\theta=1\\
  > &C=\lim_{n\rightarrow\infty}\sum_{k=1}^{n}\frac{-n}{(x_k-\bar{x})^2}=-\frac{1}{\sigma^2}\Longrightarrow A=\frac{1}{\sqrt{2\pi}\sigma}\\
  > \therefore
  > \\&f(\theta)={\frac {1}{\sigma {\sqrt {2\pi }}}}e^{-{\frac {1}{2}}\left({\frac {\theta - x }{\sigma }}\right)^{2}}\\
  > &x=1,\;\sigma^2=0,\;f(\theta)=\frac{1}{\sqrt{2\pi}}e^{{\frac{-\theta^{2}}{2}}}\Longrightarrow\Phi(\theta)={\frac {1}{\sqrt {2\pi }}}\int _{-\infty }^{\theta}e^{-y^{2}/2}\,dy
  > \end{aligned}
  > $$

-  **正态分布具有再生性**

  > 随机变量 $X_1～\mathcal{N}(\mu_1,\sigma_1^2)$ 和 $X_2～\mathcal{N}(\mu_2,\sigma_2^2)$ 相互独立，那么 $X_1+X_2～\mathcal{N}(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$



#### **均匀分布**



#### **指数分布**



### **离散分布**

#### **伯努利分布**

**伯努利分布即两点分布，离散随机变量 $X\in\{0,1\}$，随机变量 $X=1$ 的概率 $p\in[0,1]$ 随机变量 $X=0$ 的概率 $q=1-p$，均值 $E(X)=p$ 方差 $D(X)=pq=p-p^2$**

- **核心**

  > $$
  > \begin{aligned}
  > P(X;p)=&p^X(1-p)^{(1-X)}\\
  > =&p^Xq^{(1-X)}\\
  > \tilde{\mu}_3=&E\left\{\left[{\frac {X-E(X)}{\sqrt {D (X)}}}\right]^{3}\right\}\\=&p\cdot \left({\frac {q}{\sqrt {pq}}}\right)^{3}+ q\cdot \left(-{\frac {p}{\sqrt {pq}}}\right)^{3}\\=&{\frac {1}{{\sqrt {pq}}^{3} }}\left(pq^{3}-qp^{3}\right)\\=&{\frac {pq}{{\sqrt {pq}}^{3}}}(qp)= {\frac {qp}{\sqrt {pq}}}
  > \end{aligned}
  > $$



#### **二项分布**

**二项分布的本质是伯努利分布**

- **核心**

  > *离散随机变量 $X$ 独立发生 $n$ 次的伯努利事件构成二项分布 $X～\mathcal{B}(n,p)$  描述 $n$ 次中恰好有 $k$ 次 $X=1$ 的概率，均值 $E(X)=np$ 方差 $D(X)=npq=n(p-p^2)$*
  > $$
  > f(k,n,p)={\binom {n}{k}}p^{k}(1-p) ^{n-k}\\
  > {\displaystyle {\binom {n}{k}}={\frac {n!}{k!(n-k)!}}}
  > $$
  > *当 $n=1$ 时二项分布退化为伯努利分布* 



#### **泊松分布**

**泊松分布的本质是二项分布**

- **泊松分布**

  > *离散随机变量 $X$ 具有泊松分布，是二项分布的次数 $n$ 趋近无穷的情况，参数为 $\lambda >0\;\;\text{e.g.}\;\lambda=np$ 其中 $p$ 是伯努利事件发生的概率，概率质量函数* 
  > $$
  > f(k; \lambda)=\frac{\lambda^ke^{-\lambda}}{k!}
  > $$
  > *考虑一个具体问题，一天内公交车站点经停 $k$ 俩公交车的概率 $P(k)$，假设一天包含 $n$ 份单位时间，单位时间内公交车经过公交车站点停下是独立随机的且概率是 $p$，因此 $P(k)$ 可以用 ${\binom {n}{k}}p^{k}(1-p) ^{n-k}$ 描述，假设期望是 $\lambda=np$ 则可知 $p=\frac{\lambda}{n}$，单位时间需要足够精细即 $n\rightarrow\infty$ 使得单位时间内最多仅一辆公交车经停，则有* 
  > $$
  > \begin{aligned}
  > P(k)
  > =&\lim_{n\rightarrow\infty}\left\{{\binom {n}{k}}\left(\frac{\lambda}{n}\right)^{k}\left(1-\frac{\lambda}{n}\right)^{n-k}\right\}\\
  > =&\lim_{n\rightarrow\infty}\left\{\left[\frac{n(n-1)(n-2)\cdots(n-k+1)}{k!}\right]\left(\frac{\lambda}{n}\right)^{k}\left(1-\frac{\lambda}{n}\right)^{n-k}\right\}\\
  > =&\lim_{n\rightarrow\infty}\left\{\frac{\lambda^{k}}{k!}\cdot\left(1-\frac{\lambda}{n}\right)^{n}\cdot\left[\frac{n(n-1)(n-2)\cdots(n-k+1)}{n^k}\right]\cdot\left(1-\frac{\lambda}{n}\right)^{-k}\right\}\\
  > =&\lim_{n\rightarrow\infty}\frac{\lambda^{k}}{k!}\cdot\lim_{n\rightarrow\infty}\left(1-\frac{\lambda}{n}\right)^{n}\cdot\lim_{n\rightarrow\infty}\left[\frac{n(n-1)(n-2)\cdots(n-k+1)}{n^k}\right]\cdot\lim_{n\rightarrow\infty}\left(1-\frac{\lambda}{n}\right)^{-k}\\
  > =&\lim_{n\rightarrow\infty}\frac{\lambda^{k}}{k!}\cdot1\cdot1\cdot\lim_{n\rightarrow\infty}\left(1-\frac{\lambda}{n}\right)^{-k}\\
  > =&\frac{\lambda^{k}}{k!}\cdot e^{-\lambda}\\
  > \end{aligned}
  > $$
  > *当二项分布 $n$ 很大 $p$ 很小时难以计算，而 $n$ 很大 $p$ 很小通常可以使用泊松分布近似二项分布快速计算* 



### **假设检验**



### **参数估计**

#### **点估计**

#### **区间估计**



### **非参数估计**



### **随机过程**



## **信息论**

### **信息熵**

### **最大熵原理**

### **散度**



## **最优化方法**

### **最优化问题**

### **线性搜索算法**

### **启发式算法**



## **扩展**

### **深度学习**



### **广度学习**
